{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from data_processing import (load_binary_mask, \n",
    "                             extract_connected_components, \n",
    "                             match_lesions)\n",
    "from evaluation_pipeline import (evaluate_case,\n",
    "                                 evaluate_fold,\n",
    "                                 aggregate_fold_results,\n",
    "                                 evaluate_experiment)\n",
    "from exporting import (export_case_results_to_csv)\n",
    "from metrics import (compute_detection_and_lesion_metrics, \n",
    "                     compute_scan_dice)\n",
    "from statistical_tests import compare_models_wilcoxon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import defaultdict\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesion_size_threshold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Ablation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"SAM2_original\": Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_sam_automatic_lesion_wise/lesion_wise_corrective\"),\n",
    "    \"SAM2_exemplars\": Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_sam_exemplar_lesion_wise/lesion_wise_corrective\"),\n",
    "    \"MOIS_no_pointer\": Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_no_pointer_lesion_wise/lesion_wise_corrective\"),\n",
    "    \"MOIS_no_temporal\": Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_no_temporal_lesion_wise/lesion_wise_corrective\"),\n",
    "    \"MOIS_proposed\": Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_main_lesion_wise/lesion_wise_corrective\")\n",
    "}\n",
    "\n",
    "gt_root = Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/ground_truth\")\n",
    "\n",
    "sets_and_folds = {\n",
    "    \"TestSet_1\": [1],\n",
    "    \"TestSet_2\": [2],\n",
    "    \"TestSet_3\": [3]\n",
    "}\n",
    "\n",
    "gt_set_map = {\n",
    "    \"TestSet_1\": \"ValSet_1\",\n",
    "    \"TestSet_2\": \"ValSet_2\",\n",
    "    \"TestSet_3\": \"ValSet_3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = {}\n",
    "\n",
    "for model_name, pred_root in model_paths.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    result = evaluate_experiment(\n",
    "        pred_root=pred_root,\n",
    "        gt_root=gt_root,\n",
    "        sets_and_folds=sets_and_folds,\n",
    "        gt_set_map=gt_set_map,\n",
    "        size_threshold=lesion_size_threshold\n",
    "    )\n",
    "    experiment_results[model_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, result in experiment_results.items():\n",
    "    scan_dice_all = []\n",
    "    lesion_f1_all = []\n",
    "    lesion_dice_all = []\n",
    "\n",
    "    for testset in [\"TestSet_1\", \"TestSet_2\", \"TestSet_3\"]:\n",
    "        cases = result[testset][\"cases\"]\n",
    "        for case in cases:\n",
    "            # Scan Dice\n",
    "            scan_dice_all.append(case.get(\"scan_dice\", None))\n",
    "\n",
    "            # Lesion Detection F1\n",
    "            lesion_f1_all.append(case['detection_metrics']['f1_score'])\n",
    "\n",
    "            # Lesion Dice (mean of lesion-level DSCs per case)\n",
    "            lesion_dice = case.get(\"lesion_dscs\", [])\n",
    "            lesion_dice = [d for d in lesion_dice if d > 0]  # Optional: filter out 0s\n",
    "            if lesion_dice:\n",
    "                lesion_dice_all.append(np.mean(lesion_dice))\n",
    "\n",
    "    def mean_std_report(values):\n",
    "        values = [v for v in values if v is not None]\n",
    "        if len(values) == 0:\n",
    "            return \"N/A\"\n",
    "        return f\"{np.mean(values):.4f} ± {np.std(values):.4f}\"\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"  Scan Dice: {mean_std_report(scan_dice_all)}\")\n",
    "    print(f\"  Detection Rate: {mean_std_report(lesion_f1_all)}\")\n",
    "    print(f\"  Lesion Dice: {mean_std_report(lesion_dice_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_dice_scores_by_model = {}\n",
    "f1_scores_by_model = {}\n",
    "lesion_dice_scores_by_model = {}\n",
    "\n",
    "for model_name, sets in experiment_results.items():\n",
    "    \n",
    "    scan_dice_scores = []\n",
    "    f1_scores = []\n",
    "    lesion_dice_scores = []\n",
    "\n",
    "    for set_results in sets.values():\n",
    "        for case in set_results[\"cases\"]:\n",
    "            scan_dice_scores.append(case[\"scan_dice\"])\n",
    "            f1 = case[\"detection_metrics\"][\"f1_score\"]\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "            lesion_dice = case.get(\"lesion_dscs\", [])\n",
    "            \n",
    "            if lesion_dice:\n",
    "                lesion_dice = [d for d in lesion_dice if d > 0.0]\n",
    "                lesion_dice_scores.append(np.mean(lesion_dice))\n",
    "\n",
    "    scan_dice_scores_by_model[model_name] = scan_dice_scores\n",
    "    f1_scores_by_model[model_name] = f1_scores\n",
    "    lesion_dice_scores_by_model[model_name] = lesion_dice_scores\n",
    "\n",
    "\n",
    "# Scan Dice\n",
    "wilcoxon_results_dice = compare_models_wilcoxon(scan_dice_scores_by_model, alpha=0.05, correction='bonferroni')\n",
    "df_dice = pd.DataFrame(wilcoxon_results_dice)\n",
    "print(\"\\n=== Wilcoxon: Scan Dice ===\")\n",
    "display(df_dice)\n",
    "\n",
    "# Lesion Detection F1\n",
    "wilcoxon_results_f1 = compare_models_wilcoxon(f1_scores_by_model, alpha=0.05, correction='bonferroni')\n",
    "df_f1 = pd.DataFrame(wilcoxon_results_f1)\n",
    "print(\"\\n=== Wilcoxon: Lesion Detection F1 ===\")\n",
    "display(df_f1)\n",
    "\n",
    "# Lesion-wise Dice\n",
    "wilcoxon_results_lesion = compare_models_wilcoxon(lesion_dice_scores_by_model, alpha=0.05, correction='bonferroni')\n",
    "df_lesion = pd.DataFrame(wilcoxon_results_lesion)\n",
    "print(\"\\n=== Wilcoxon: Lesion-wise Dice ===\")\n",
    "display(df_lesion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_1.json\", \"w\") as f:\n",
    "    json.dump(experiment_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Validating the number of the prompted lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth mapping\n",
    "gt_root = Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/ground_truth\")\n",
    "gt_set_map = {\n",
    "    \"TestSet_1\": \"ValSet_1\",\n",
    "    \"TestSet_2\": \"ValSet_2\",\n",
    "    \"TestSet_3\": \"ValSet_3\"\n",
    "}\n",
    "\n",
    "# Set and fold info\n",
    "sets_and_folds = {\n",
    "    \"TestSet_1\": [1],\n",
    "    \"TestSet_2\": [2],\n",
    "    \"TestSet_3\": [3]\n",
    "}\n",
    "\n",
    "# List of models for 1–10 prompted lesions\n",
    "model_config = {\n",
    "    f\"num_ex_{i}\": Path(\n",
    "        f\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/\"\n",
    "        f\"evaluation/results/predictions/MOIS_SAM2_stream_exemplars_num_ex_{i}/lesion_wise_corrective\"\n",
    "    )\n",
    "    for i in range(1, 11)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the evaluation for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_counts = list(range(1, 11))\n",
    "\n",
    "experiment_results_prompted = {}\n",
    "\n",
    "for num in prompt_counts:\n",
    "    model_name = f\"MOIS_SAM2_num_{num}\"\n",
    "    model_path = f\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions/MOIS_SAM2_stream_exemplars_num_ex_{num}/lesion_wise_corrective\"\n",
    "\n",
    "    print(f\"\\n=== Evaluating model: {model_name} ===\")\n",
    "    result = evaluate_experiment(\n",
    "        pred_root=model_path,\n",
    "        gt_root=gt_root,\n",
    "        sets_and_folds={\n",
    "            \"TestSet_1\": [1],\n",
    "            \"TestSet_2\": [2],\n",
    "            \"TestSet_3\": [3],\n",
    "        },\n",
    "        gt_set_map={\n",
    "            \"TestSet_1\": \"ValSet_1\",\n",
    "            \"TestSet_2\": \"ValSet_2\",\n",
    "            \"TestSet_3\": \"ValSet_3\",\n",
    "        },\n",
    "        size_threshold=lesion_size_threshold\n",
    "    )\n",
    "\n",
    "    experiment_results_prompted[num] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_2.json\", \"w\") as f:\n",
    "    json.dump(experiment_results_prompted, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metrics\n",
    "num_prompts = sorted(int(k) for k in experiment_results_prompted.keys())\n",
    "scan_dice_means, scan_dice_stds = [], []\n",
    "lesion_dice_means, lesion_dice_stds = [], []\n",
    "f1_means, f1_stds = [], []\n",
    "\n",
    "for n in num_prompts:\n",
    "    sets = experiment_results_prompted[n]\n",
    "\n",
    "    # Extract metrics from all three folds\n",
    "    scan = [sets[s][\"aggregated\"][\"scan_dice\"] for s in sets]\n",
    "    lesion = [sets[s][\"aggregated\"][\"lesion_dice\"] for s in sets]\n",
    "    f1 = [sets[s][\"aggregated\"][\"lesion_detection_f1\"] for s in sets]\n",
    "\n",
    "    def mean_std(metrics):\n",
    "        valid = [m for m in metrics if m[\"mean\"] is not None]\n",
    "        if not valid:\n",
    "            return 0.0, 0.0\n",
    "        mean = np.mean([m[\"mean\"] for m in valid])\n",
    "        std = np.mean([m[\"std\"] for m in valid])\n",
    "        return mean, std\n",
    "\n",
    "    scan_mean, scan_std = mean_std(scan)\n",
    "    lesion_mean, lesion_std = mean_std(lesion)\n",
    "    f1_mean, f1_std = mean_std(f1)\n",
    "\n",
    "    scan_dice_means.append(scan_mean)\n",
    "    scan_dice_stds.append(scan_std)\n",
    "    lesion_dice_means.append(lesion_mean)\n",
    "    lesion_dice_stds.append(lesion_std)\n",
    "    f1_means.append(f1_mean)\n",
    "    f1_stds.append(f1_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare x values (number of prompts)\n",
    "x = num_prompts\n",
    "\n",
    "# Helper to create upper/lower bounds\n",
    "def bounds(mean_list, std_list):\n",
    "    lower = [max(m - s, 0) for m, s in zip(mean_list, std_list)]\n",
    "    upper = [min(m + s, 1) for m, s in zip(mean_list, std_list)]\n",
    "    return lower, upper\n",
    "\n",
    "# Compute bounds for shaded areas\n",
    "scan_dice_lower, scan_dice_upper = bounds(scan_dice_means, scan_dice_stds)\n",
    "lesion_dice_lower, lesion_dice_upper = bounds(lesion_dice_means, lesion_dice_stds)\n",
    "f1_lower, f1_upper = bounds(f1_means, f1_stds)\n",
    "\n",
    "# Create subplots (1 row x 3 columns)\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "    \"Scan-wise Dice vs #Prompts\", \n",
    "    \"Lesion-wise Dice vs #Prompts\", \n",
    "    \"Lesion Detection F1 vs #Prompts\"\n",
    "))\n",
    "\n",
    "# --- Scan Dice\n",
    "fig.add_trace(go.Scatter(x=x, y=scan_dice_means, mode='lines+markers', name=\"Scan Dice\",\n",
    "                         line=dict(color='blue')), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x + x[::-1], y=scan_dice_upper + scan_dice_lower[::-1],\n",
    "                         fill='toself', fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "                         line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=1, col=1)\n",
    "\n",
    "# --- Lesion Dice\n",
    "fig.add_trace(go.Scatter(x=x, y=lesion_dice_means, mode='lines+markers', name=\"Lesion Dice\",\n",
    "                         line=dict(color='green')), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x + x[::-1], y=lesion_dice_upper + lesion_dice_lower[::-1],\n",
    "                         fill='toself', fillcolor='rgba(0, 255, 0, 0.2)',\n",
    "                         line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=1, col=2)\n",
    "\n",
    "# --- F1 Score\n",
    "fig.add_trace(go.Scatter(x=x, y=f1_means, mode='lines+markers', name=\"Lesion F1\",\n",
    "                         line=dict(color='red')), row=1, col=3)\n",
    "fig.add_trace(go.Scatter(x=x + x[::-1], y=f1_upper + f1_lower[::-1],\n",
    "                         fill='toself', fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                         line=dict(color='rgba(255,255,255,0)'), showlegend=False), row=1, col=3)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=500, width=1200, \n",
    "                  title_text=\"Segmentation Performance vs Prompt Count\", \n",
    "                  showlegend=False)\n",
    "\n",
    "# Uniform axis settings\n",
    "for i in range(1, 4):\n",
    "    fig.update_xaxes(range=[0, 10], dtick=1, row=1, col=i)\n",
    "    fig.update_yaxes(range=[0.0, 1.0], row=1, col=i,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_dice_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesion_dice_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Ensure kaleido is installed: pip install kaleido\n",
    "\n",
    "# Function to compute bounds\n",
    "def bounds(mean_list, std_list):\n",
    "    lower = [max(m - s, 0) for m, s in zip(mean_list, std_list)]\n",
    "    upper = [min(m + s, 1) for m, s in zip(mean_list, std_list)]\n",
    "    return lower, upper\n",
    "\n",
    "# Bounds\n",
    "scan_lower, scan_upper = bounds(scan_dice_means, scan_dice_stds)\n",
    "lesion_lower, lesion_upper = bounds(lesion_dice_means, lesion_dice_stds)\n",
    "f1_lower, f1_upper = bounds(f1_means, f1_stds)\n",
    "\n",
    "# Helper function to create and save each plot\n",
    "def save_plot(x, y_mean, y_lower, y_upper, line_color, fill_color, filename):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Main line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x, y=y_mean,\n",
    "        line=dict(color=line_color, width=3),\n",
    "        marker=dict(size=6),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    # Fill area for std\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x + x[::-1],\n",
    "        y=y_upper + y_lower[::-1],\n",
    "        fill='toself',\n",
    "        fillcolor=fill_color,\n",
    "        line=dict(color='rgba(255,255,255,0)'),\n",
    "        hoverinfo=\"skip\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=400, height=400,\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        xaxis=dict(range=[0, 11], showticklabels=False, showgrid=True, zeroline=True),\n",
    "        yaxis=dict(range=[0.0, 1.0], showticklabels=False, showgrid=True, zeroline=True),\n",
    "    )\n",
    "\n",
    "    # Save image at 400 DPI\n",
    "    pio.write_image(fig, filename, width=400, height=400, scale=4)\n",
    "\n",
    "# Save all three plots\n",
    "save_plot(\n",
    "    x=num_prompts,\n",
    "    y_mean=scan_dice_means,\n",
    "    y_lower=scan_lower,\n",
    "    y_upper=scan_upper,\n",
    "    line_color='rgb(255, 0, 0)',\n",
    "    fill_color='rgba(255, 0, 0, 0.2)',\n",
    "    filename='scanwise_dice.png'\n",
    ")\n",
    "\n",
    "save_plot(\n",
    "    x=num_prompts,\n",
    "    y_mean=lesion_dice_means,\n",
    "    y_lower=lesion_lower,\n",
    "    y_upper=lesion_upper,\n",
    "    line_color='rgb(0, 0, 255)',\n",
    "    fill_color='rgba(0, 0, 255, 0.2)',\n",
    "    filename='lesionwise_dice.png'\n",
    ")\n",
    "\n",
    "save_plot(\n",
    "    x=num_prompts,\n",
    "    y_mean=f1_means,\n",
    "    y_lower=f1_lower,\n",
    "    y_upper=f1_upper,\n",
    "    line_color='rgb(0, 128, 0)',\n",
    "    fill_color='rgba(0, 128, 0, 0.2)',\n",
    "    filename='lesion_detection_f1.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Benchmarking against other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root paths\n",
    "pred_root_base = Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions\")\n",
    "gt_root = Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/ground_truth\")\n",
    "\n",
    "# Model folder mapping\n",
    "model_paths = {\n",
    "    \"UNet\": pred_root_base / \"unet\" / \"global_non_corrective\",\n",
    "    \"nnUNet\": pred_root_base / \"nnunet\" / \"global_non_corrective\",\n",
    "    \"DINs\": pred_root_base / \"DINs\" / \"lesion_wise_corrective\",\n",
    "    \"SW-FastEdit\": pred_root_base / \"SW-FastEdit\" / \"lesion_wise_corrective\",\n",
    "    \"SAM2\": pred_root_base / \"SAM2\" / \"lesion_wise_corrective\",\n",
    "    \"VISTA3D (Interactive)\": pred_root_base / \"VISTA\" / \"lesion_wise_corrective\",\n",
    "    \"VISTA3D (Automatic)\": pred_root_base / \"VISTA\" / \"global_non_corrective\",\n",
    "    \"MOIS-SAM2\": pred_root_base / \"MOIS_SAM2\" / \"lesion_wise_corrective\"\n",
    "}\n",
    "\n",
    "# Sets and folds\n",
    "sets_and_folds = {\n",
    "    \"TestSet_1\": [1, 2, 3],\n",
    "    \"TestSet_2\": [1, 2, 3],\n",
    "    \"TestSet_3\": [1, 2, 3],\n",
    "    \"TestSet_4\": [1, 2, 3],\n",
    "}\n",
    "\n",
    "# GT folder mapping\n",
    "gt_set_map = {\n",
    "    \"TestSet_1\": \"TestSet_1\",\n",
    "    \"TestSet_2\": \"TestSet_2\",\n",
    "    \"TestSet_3\": \"TestSet_3\",\n",
    "    \"TestSet_4\": \"TestSet_4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiment_results = {}\n",
    "for model_name, model_path in model_paths.items():\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    experiment_result = evaluate_experiment(\n",
    "        pred_root=model_path,\n",
    "        gt_root=gt_root,\n",
    "        sets_and_folds=sets_and_folds,\n",
    "        gt_set_map=gt_set_map,\n",
    "        size_threshold=lesion_size_threshold\n",
    "    )\n",
    "    all_experiment_results[model_name] = experiment_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_benchmark_summary(results_dict):\n",
    "    print(\"\\n=== Benchmarking Summary Per Dataset ===\")\n",
    "    for dataset in [\"TestSet_1\", \"TestSet_2\", \"TestSet_3\", \"TestSet_4\"]:\n",
    "        print(f\"\\n--- {dataset} ---\")\n",
    "        for model_name, result in results_dict.items():\n",
    "            agg = result.get(dataset, {}).get(\"aggregated\", {})\n",
    "            scan_dice = agg.get(\"scan_dice\", {\"mean\": None, \"std\": None})\n",
    "            f1 = agg.get(\"lesion_detection_f1\", {\"mean\": None, \"std\": None})\n",
    "            lesion_dice = agg.get(\"lesion_dice\", {\"mean\": None, \"std\": None})\n",
    "\n",
    "            print(f\"{model_name:25s} | \"\n",
    "                  f\"Scan DSC: {scan_dice['mean']:.3f} ± {scan_dice['std']:.3f} | \"\n",
    "                  f\"F1: {f1['mean']:.3f} ± {f1['std']:.3f} | \"\n",
    "                  f\"Lesion DSC: {lesion_dice['mean']:.3f} ± {lesion_dice['std']:.3f}\")\n",
    "\n",
    "print_benchmark_summary(all_experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_experiment_results.json\", \"w\") as f:\n",
    "    json.dump(all_experiment_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "metric_scores_by_testset = {\n",
    "    'scan_dice': defaultdict(dict),\n",
    "    'lesion_dscs': defaultdict(dict),\n",
    "    'lesion_f1': defaultdict(dict),\n",
    "}\n",
    "\n",
    "# Extract scores\n",
    "for model, testsets in all_experiment_results.items():\n",
    "    for testset, data in testsets.items():\n",
    "        case_results = data.get(\"cases\", [])\n",
    "        scan_scores, lesion_scores, f1_scores = [], [], []\n",
    "        for case in case_results:\n",
    "            scan_scores.append(case.get(\"scan_dice\", None))\n",
    "\n",
    "            lesion_dice = case.get(\"lesion_dscs\", [])\n",
    "            lesion_scores.append(np.mean([d for d in lesion_dice if d > 0.0]) if lesion_dice else None)\n",
    "\n",
    "            f1 = case.get(\"detection_metrics\", {}).get(\"f1_score\", None)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Clean Nones\n",
    "        metric_scores_by_testset[\"scan_dice\"][testset][model] = [v for v in scan_scores if v is not None]\n",
    "        metric_scores_by_testset[\"lesion_dscs\"][testset][model] = [v for v in lesion_scores if v is not None]\n",
    "        metric_scores_by_testset[\"lesion_f1\"][testset][model] = [v for v in f1_scores if v is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wilcoxon_vs_reference(metric_data, reference_model='MOIS-SAM2', alpha=0.05):\n",
    "    comparisons = []\n",
    "    for testset, model_scores in metric_data.items():\n",
    "        if reference_model not in model_scores:\n",
    "            continue\n",
    "        ref_scores = model_scores[reference_model]\n",
    "        for model, scores in model_scores.items():\n",
    "            if model == reference_model or len(ref_scores) != len(scores):\n",
    "                continue\n",
    "            try:\n",
    "                stat, p = wilcoxon(ref_scores, scores)\n",
    "            except ValueError:\n",
    "                p = 1.0\n",
    "            comparisons.append({\n",
    "                \"testset\": testset,\n",
    "                \"reference\": reference_model,\n",
    "                \"compared_to\": model,\n",
    "                \"p_value\": p,\n",
    "                \"significant\": p < alpha\n",
    "            })\n",
    "    return comparisons\n",
    "\n",
    "scan_results = wilcoxon_vs_reference(metric_scores_by_testset[\"scan_dice\"])\n",
    "lesion_results = wilcoxon_vs_reference(metric_scores_by_testset[\"lesion_dscs\"])\n",
    "f1_results = wilcoxon_vs_reference(metric_scores_by_testset[\"lesion_f1\"])\n",
    "\n",
    "df_scan = pd.DataFrame(scan_results).assign(metric=\"Scan Dice\")\n",
    "df_lesion = pd.DataFrame(lesion_results).assign(metric=\"Lesion Dice\")\n",
    "df_f1 = pd.DataFrame(f1_results).assign(metric=\"Lesion Detection F1\")\n",
    "\n",
    "# Combine all\n",
    "wilcoxon_df = pd.concat([df_scan, df_lesion, df_f1], ignore_index=True)\n",
    "display(wilcoxon_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain shift analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data: {model: {testset: [scan_dice values]}}\n",
    "def extract_scan_dice_scores(data):\n",
    "    scan_dice_scores = {}\n",
    "    for model_name, model_data in data.items():\n",
    "        scan_dice_scores[model_name] = {}\n",
    "        for test_set in [\"TestSet_1\", \"TestSet_2\", \"TestSet_3\", \"TestSet_4\"]:\n",
    "            cases = model_data.get(test_set, {}).get(\"cases\", [])\n",
    "            scan_dice_scores[model_name][test_set] = [\n",
    "                case[\"scan_dice\"] for case in cases if \"scan_dice\" in case\n",
    "            ]\n",
    "    return scan_dice_scores\n",
    "\n",
    "# Replace this with your loaded dictionary\n",
    "# all_experiment_results = ...\n",
    "\n",
    "model_testset_dice = extract_scan_dice_scores(all_experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the scan_dice scores into a long-format DataFrame\n",
    "rows = []\n",
    "for model, testsets in model_testset_dice.items():\n",
    "    if model in ['nnUNet', 'DINs', 'SAM2', 'MOIS-SAM2']:\n",
    "        for testset, scores in testsets.items():\n",
    "            for score in scores:\n",
    "                rows.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Test Set\": testset,\n",
    "                    \"Scan-wise DSC\": score\n",
    "                })\n",
    "\n",
    "df_box = pd.DataFrame(rows)\n",
    "\n",
    "# Create box plot\n",
    "fig = px.box(\n",
    "    df_box,\n",
    "    x=\"Model\",\n",
    "    y=\"Scan-wise DSC\",\n",
    "    color=\"Test Set\",\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(range=[0.0, 1.0], showticklabels=False),\n",
    "    xaxis=dict(showticklabels=False),\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    boxmode='group',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "    # Save image at 400 DPI\n",
    "pio.write_image(fig, \"domain_generalization.png\", scale=4)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Interaction efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize after kernel reset\n",
    "# Define the test sets and folds\n",
    "sets_and_folds = {\n",
    "    \"TestSet_1\": [1, 2, 3],\n",
    "    \"TestSet_2\": [1, 2, 3],\n",
    "    \"TestSet_3\": [1, 2, 3],\n",
    "    \"TestSet_4\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Ground truth folder mapping\n",
    "gt_set_map = {\n",
    "    \"TestSet_1\": \"TestSet_1\",\n",
    "    \"TestSet_2\": \"TestSet_2\",\n",
    "    \"TestSet_3\": \"TestSet_3\",\n",
    "    \"TestSet_4\": \"TestSet_4\"\n",
    "}\n",
    "\n",
    "# Base path\n",
    "base_path = Path(\"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/predictions\")\n",
    "\n",
    "# Model folders\n",
    "sam2_paths = {\n",
    "    f\"SAM2_{i}\": base_path / f\"SAM2_num_lesions_{i}\" / \"lesion_wise_corrective\"\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "mois_sam2_paths = {\n",
    "    f\"MOIS_SAM2_{i}\": base_path / f\"MOIS_SAM2_num_lesions_{i}\" / \"lesion_wise_corrective\"\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "interaction_models = {**sam2_paths, **mois_sam2_paths}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all 20 models (SAM2 and MOIS-SAM2 with 1–10 prompts)\n",
    "interaction_results = {}\n",
    "\n",
    "for model_name, model_path in interaction_models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    result = evaluate_experiment(\n",
    "        pred_root=model_path,\n",
    "        gt_root=gt_root,\n",
    "        sets_and_folds=sets_and_folds,\n",
    "        gt_set_map=gt_set_map,\n",
    "        size_threshold=lesion_size_threshold\n",
    "    )\n",
    "    interaction_results[model_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_4.json\", \"w\") as f:\n",
    "    json.dump(interaction_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation of the scan-wise DSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_4.json\", \"r\") as f:\n",
    "    interaction_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare structure: testset → model → list of (prompt_count, mean, std)\n",
    "scan_dice_summary = defaultdict(lambda: {\"SAM2\": [], \"MOIS_SAM2\": []})\n",
    "\n",
    "for model_key, model_result in interaction_results.items():\n",
    "    if model_key.startswith(\"MOIS_SAM2_\"):\n",
    "        model_type = \"MOIS_SAM2\"\n",
    "        prompt_count = int(model_key.replace(\"MOIS_SAM2_\", \"\"))\n",
    "    elif model_key.startswith(\"SAM2_\"):\n",
    "        model_type = \"SAM2\"\n",
    "        prompt_count = int(model_key.replace(\"SAM2_\", \"\"))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for testset in model_result:\n",
    "        agg = model_result[testset][\"aggregated\"]\n",
    "        mean = agg[\"scan_dice\"][\"mean\"]\n",
    "        std = agg[\"scan_dice\"][\"std\"]\n",
    "        scan_dice_summary[testset][model_type].append((prompt_count, mean, std))\n",
    "\n",
    "# Sort entries by prompt count\n",
    "for testset in scan_dice_summary:\n",
    "    for model in [\"SAM2\", \"MOIS_SAM2\"]:\n",
    "        scan_dice_summary[testset][model] = sorted(scan_dice_summary[testset][model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scan_dice_single_testset(scan_dice_summary, testset_name):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for model in [\"SAM2\", \"MOIS_SAM2\"]:\n",
    "        data = scan_dice_summary[testset_name][model]\n",
    "        print(\"Model: \", model)\n",
    "        \n",
    "        if not data:\n",
    "            continue\n",
    "        x = [d[0] for d in data]\n",
    "        y = [d[1] for d in data]\n",
    "        print(y)\n",
    "        std = [d[2] for d in data]\n",
    "\n",
    "        # Mean curve\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x, y=y, mode=\"lines+markers\",\n",
    "            name=model,\n",
    "            line=dict(shape='spline'),\n",
    "        ))\n",
    "\n",
    "        # Shaded error band\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x + x[::-1],\n",
    "            y=[m + s for m, s in zip(y, std)] + [m - s for m, s in zip(y[::-1], std[::-1])],\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0,100,200,0.1)' if model == \"SAM2\" else 'rgba(0,200,100,0.1)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Scan-wise DSC vs Prompted Lesions – {testset_name}\",\n",
    "        xaxis=dict(title=\"Number of Prompted Lesions\", range=[0, 10], dtick=1),\n",
    "        yaxis=dict(title=\"Scan-wise DSC\", range=[0.0, 1.0]),\n",
    "        height=500,\n",
    "        width=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_scan_dice_single_testset(scan_dice_summary, testset_name, save_path):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for model in [\"SAM2\", \"MOIS_SAM2\"]:\n",
    "        data = scan_dice_summary[testset_name][model]\n",
    "        if not data:\n",
    "            continue\n",
    "        x = [d[0] for d in data]\n",
    "        y = [d[1] for d in data]\n",
    "        std = [d[2] for d in data]\n",
    "\n",
    "        # Choose color for MOIS-SAM2 and SAM2\n",
    "        color = 'rgba(0,100,200,1)' if model == \"SAM2\" else 'rgba(0,200,100,1)'\n",
    "\n",
    "        # Mean curve\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x, y=y, mode=\"lines+markers\",\n",
    "            name=model,\n",
    "            line=dict(shape='spline', color=color, width=3),\n",
    "            marker=dict(color=color),\n",
    "        ))\n",
    "\n",
    "        # Shaded error band\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x + x[::-1],\n",
    "            y=[m + s for m, s in zip(y, std)] + [m - s for m, s in zip(y[::-1], std[::-1])],\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0,100,200,0.1)' if model == \"SAM2\" else 'rgba(0,200,100,0.1)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(range=[0, 11], dtick=2, showticklabels=False),\n",
    "        yaxis=dict(range=[0.0, 1.0], showticklabels=False),\n",
    "        width=400, height=400,\n",
    "        showlegend=False,\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "    )\n",
    "\n",
    "    # Save figure at 400 DPI\n",
    "    pio.write_image(fig, save_path, format=\"png\", width=400, height=400,scale=4)  # scale=4 ~ 400 DPI\n",
    "\n",
    "    print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_1\", 'exp_4_ts1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_2\", 'exp_4_ts2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_3\", 'exp_4_ts3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scan_dice_single_testset(scan_dice_summary, \"TestSet_4\", 'exp_4_ts4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_iseg_benchmark_torch_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
