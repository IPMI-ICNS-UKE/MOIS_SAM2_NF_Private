{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import SimpleITK as sitk\n",
    "from collections import defaultdict\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = \"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/images\"\n",
    "mask_root = \"/home/gkolokolnikov/PhD_project/nf_segmentation_interactive/NFInteractiveSegmentationBenchmarkingPrivate/evaluation/results/ground_truth\"\n",
    "\n",
    "dataset_names = [\"TrainingSet\", \"TestSet_1\", \"TestSet_2\", \"TestSet_3\", \"TestSet_4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date time extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime from description\n",
    "def extract_datetime_from_string(meta_str):\n",
    "    match = re.search(r'\\d{1,2}-[A-Za-z]{3}-\\d{4} \\d{1,2}:\\d{1,2}:\\d{1,2}(?:\\.\\d+)?', meta_str)\n",
    "    if match:\n",
    "        try:\n",
    "            return datetime.strptime(match.group(0), \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            return datetime.strptime(match.group(0), \"%d-%b-%Y %H:%M:%S\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_name):\n",
    "    image_dir = os.path.join(image_root, dataset_name)\n",
    "    mask_dir = os.path.join(mask_root, dataset_name)\n",
    "\n",
    "    acquisition_dates = []\n",
    "    slice_counts = []\n",
    "    inplane_sizes = []\n",
    "    volumes = []\n",
    "    spacings = []\n",
    "    lesion_counts = []\n",
    "\n",
    "    file_list = [f for f in os.listdir(image_dir) if f.endswith(\".nii.gz\")]\n",
    "\n",
    "    for fname in tqdm(file_list, desc=f\"Processing {dataset_name}\"):\n",
    "        try:\n",
    "            image_path = os.path.join(image_dir, fname)\n",
    "            mask_path = os.path.join(mask_dir, fname)\n",
    "            \n",
    "            img = sitk.ReadImage(image_path)\n",
    "            mask = sitk.ReadImage(mask_path)\n",
    "\n",
    "            # Acquisition date from metadata\n",
    "            desc = \"\"\n",
    "            for key in [\"descrip\"]:\n",
    "                \n",
    "                if img.HasMetaDataKey(key):\n",
    "                    desc = img.GetMetaData(key)\n",
    "                    break\n",
    "            dt = extract_datetime_from_string(desc)\n",
    "            if dt:\n",
    "                acquisition_dates.append(dt)\n",
    "\n",
    "            # Shape and spacing\n",
    "            x, y, z = img.GetSize()\n",
    "            spacing = img.GetSpacing()\n",
    "            sp_x, sp_y, sp_z = spacing\n",
    "            slice_counts.append(z)\n",
    "            inplane_sizes.append(tuple(sorted((x, y))))  # (smaller, larger)\n",
    "\n",
    "            # Volume in cm³\n",
    "            mask_array = sitk.GetArrayFromImage(mask)\n",
    "            tumor_voxels = np.sum(mask_array > 0)\n",
    "            voxel_volume_cm3 = np.prod(spacing) / 1000\n",
    "            volumes.append(tumor_voxels * voxel_volume_cm3)\n",
    "            spacings.append(tuple(sorted((sp_x, sp_y, sp_z))))\n",
    "\n",
    "            # Lesion count\n",
    "            labeled, num_lesions = label(mask_array > 0)\n",
    "            lesion_counts.append(num_lesions)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{dataset_name}] Error processing {fname}: {e}\")\n",
    "\n",
    "    # Summary\n",
    "    def iqr_stats(arr):\n",
    "        median = np.median(arr)\n",
    "        q1 = np.percentile(arr, 25)\n",
    "        q3 = np.percentile(arr, 75)\n",
    "        return median, q3 - q1\n",
    "\n",
    "    # Compute in-plane split\n",
    "    small_dims = [s[0] for s in inplane_sizes]\n",
    "    large_dims = [s[1] for s in inplane_sizes]\n",
    "    small_sp = [s[0] for s in spacings]\n",
    "    medium_sp = [s[1] for s in spacings]\n",
    "    large_sp = [s[2] for s in spacings]\n",
    "\n",
    "    # Compute stats\n",
    "    slice_med, slice_iqr = iqr_stats(slice_counts)\n",
    "    small_med, small_iqr = iqr_stats(small_dims)\n",
    "    large_med, large_iqr = iqr_stats(large_dims)\n",
    "    vol_med, vol_iqr = iqr_stats(volumes)\n",
    "    sp_s_med, sp_s_iqr = iqr_stats(small_sp)\n",
    "    sp_m_med, sp_m_iqr = iqr_stats(medium_sp)\n",
    "    sp_l_med, sp_l_iqr = iqr_stats(large_sp)\n",
    "    lesion_med, lesion_iqr = iqr_stats(lesion_counts)\n",
    "\n",
    "    # Summary print\n",
    "    print(f\"\\n==== {dataset_name} ====\")\n",
    "    print(f\"Cases: {len(slice_counts)}\")\n",
    "    if acquisition_dates:\n",
    "        print(f\"Acquisition Date Range: {min(acquisition_dates).date()} to {max(acquisition_dates).date()}\")\n",
    "    else:\n",
    "        print(\"Acquisition Date Range: N/A\")\n",
    "    \n",
    "    min_small = min(s[0] for s in inplane_sizes)\n",
    "    max_small = max(s[0] for s in inplane_sizes)\n",
    "    min_large = min(s[1] for s in inplane_sizes)\n",
    "    max_large = max(s[1] for s in inplane_sizes)\n",
    "    \n",
    "\n",
    "    print(f\"Slices: {min(slice_counts)} to {max(slice_counts)}; {slice_med:.1f} ± {slice_iqr:.1f}\")\n",
    "    print(f\"In-plane size (smaller dim): {min_small} to {max_small}; {small_med:.1f} ± {small_iqr:.1f}\")\n",
    "    print(f\"In-plane size (larger dim): {min_large} to {max_large}; {large_med:.1f} ± {large_iqr:.1f}\")\n",
    "    print(f\"Spacing: {sp_s_med:.2f} ± {sp_s_iqr:.2f}; {sp_m_med:.2f} ± {sp_m_iqr:.2f}, {sp_l_med:.2f} ± {sp_l_iqr:.2f}\")\n",
    "    print(f\"Tumor volume (cm³): {vol_med:.2f} ± {vol_iqr:.2f}\")\n",
    "    print(f\"Number of lesions: {lesion_med:.1f} ± {lesion_iqr:.1f}\")\n",
    "    print(f\"\\n==== ==== ==== ==== ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for each dataset\n",
    "for dataset in dataset_names:\n",
    "    analyze_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_iseg_benchmark_torch_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
